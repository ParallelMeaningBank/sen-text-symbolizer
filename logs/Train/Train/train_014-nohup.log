Preparing WMT data in ../models/tmp_014
Creating vocabulary ../models/tmp_014/vocab1000.sym from data ../models/tmp_014/pmb_2016-12-14_silver_plus_artificial.train.sym
  processing line 100000
  processing line 200000
  processing line 300000
  processing line 400000
  processing line 500000
  processing line 600000
  processing line 700000
  processing line 800000
  processing line 900000
  processing line 1000000
  processing line 1100000
  processing line 1200000
  processing line 1300000
  processing line 1400000
  processing line 1500000
  processing line 1600000
  processing line 1700000
  processing line 1800000
  processing line 1900000
  processing line 2000000
  processing line 2100000
  processing line 2200000
Creating vocabulary ../models/tmp_014/vocab1000.word from data ../models/tmp_014/pmb_2016-12-14_silver_plus_artificial.train.word
  processing line 100000
  processing line 200000
  processing line 300000
  processing line 400000
  processing line 500000
  processing line 600000
  processing line 700000
  processing line 800000
  processing line 900000
  processing line 1000000
  processing line 1100000
  processing line 1200000
  processing line 1300000
  processing line 1400000
  processing line 1500000
  processing line 1600000
  processing line 1700000
  processing line 1800000
  processing line 1900000
  processing line 2000000
  processing line 2100000
  processing line 2200000
Tokenizing data in ../models/tmp_014/pmb_2016-12-14_silver_plus_artificial.train.sym
  tokenizing line 100000
  tokenizing line 200000
  tokenizing line 300000
  tokenizing line 400000
  tokenizing line 500000
  tokenizing line 600000
  tokenizing line 700000
  tokenizing line 800000
  tokenizing line 900000
  tokenizing line 1000000
  tokenizing line 1100000
  tokenizing line 1200000
  tokenizing line 1300000
  tokenizing line 1400000
  tokenizing line 1500000
  tokenizing line 1600000
  tokenizing line 1700000
  tokenizing line 1800000
  tokenizing line 1900000
  tokenizing line 2000000
  tokenizing line 2100000
  tokenizing line 2200000
Tokenizing data in ../models/tmp_014/pmb_2016-12-14_silver_plus_artificial.train.word
  tokenizing line 100000
  tokenizing line 200000
  tokenizing line 300000
  tokenizing line 400000
  tokenizing line 500000
  tokenizing line 600000
  tokenizing line 700000
  tokenizing line 800000
  tokenizing line 900000
  tokenizing line 1000000
  tokenizing line 1100000
  tokenizing line 1200000
  tokenizing line 1300000
  tokenizing line 1400000
  tokenizing line 1500000
  tokenizing line 1600000
  tokenizing line 1700000
  tokenizing line 1800000
  tokenizing line 1900000
  tokenizing line 2000000
  tokenizing line 2100000
  tokenizing line 2200000
Tokenizing data in ../models/tmp_014/pmb_2016-12-14_silver_plus_artificial.dev.sym
  tokenizing line 100000
  tokenizing line 200000
  tokenizing line 300000
  tokenizing line 400000
  tokenizing line 500000
Tokenizing data in ../models/tmp_014/pmb_2016-12-14_silver_plus_artificial.dev.word
  tokenizing line 100000
  tokenizing line 200000
  tokenizing line 300000
  tokenizing line 400000
  tokenizing line 500000
wrd_train: ../models/tmp_014/pmb_2016-12-14_silver_plus_artificial.train.ids1000.word
sym_train: ../models/tmp_014/pmb_2016-12-14_silver_plus_artificial.train.ids1000.sym
wrd_dev: ../models/tmp_014/pmb_2016-12-14_silver_plus_artificial.dev.ids1000.word
sym_dev: ../models/tmp_014/pmb_2016-12-14_silver_plus_artificial.dev.ids1000.sym

Creating 3 layers of 1024 units.
Created model with fresh parameters.
Reading development and training data (limit: 0).
  reading data line 100000
  reading data line 200000
  reading data line 300000
  reading data line 400000
  reading data line 500000
  reading data line 100000
  reading data line 200000
  reading data line 300000
  reading data line 400000
  reading data line 500000
  reading data line 600000
  reading data line 700000
  reading data line 800000
  reading data line 900000
  reading data line 1000000
  reading data line 1100000
  reading data line 1200000
  reading data line 1300000
  reading data line 1400000
  reading data line 1500000
  reading data line 1600000
  reading data line 1700000
  reading data line 1800000
  reading data line 1900000
  reading data line 2000000
  reading data line 2100000
  reading data line 2200000
Reading completed. Now estimating buckets and sizes.
_buckets[(13, 27), (36, 35), (75, 8), (85, 70)] 
train_bucket_sizes: [1461581, 654706, 124490, 831] 
train_total_size: 2241608.0
global step 400 learning rate 0.3000 step-time 6.97 perplexity 19.25
  eval: bucket 0 perplexity 8.30
  eval: bucket 1 perplexity 11.73
  eval: bucket 2 perplexity 38.89
  eval: bucket 3 perplexity 32.63
max_preplexity_count: 0 
 prexs: [8.299495460278353, 11.732004869510314, 38.88986589315802, 32.63487830933069]
global step 800 learning rate 0.3000 step-time 6.75 perplexity 6.21
  eval: bucket 0 perplexity 4.14
  eval: bucket 1 perplexity 6.72
  eval: bucket 2 perplexity 14.35
  eval: bucket 3 perplexity 30.34
max_preplexity_count: 0 
 prexs: [4.140841029124491, 6.724911064497403, 14.347601735878651, 30.344934608166756]
global step 1200 learning rate 0.3000 step-time 6.42 perplexity 4.71
  eval: bucket 0 perplexity 3.37
  eval: bucket 1 perplexity 6.47
  eval: bucket 2 perplexity 6.88
  eval: bucket 3 perplexity 28.57
max_preplexity_count: 0 
 prexs: [3.3729746487996364, 6.473255297220382, 6.883671315649459, 28.569882285146242]
global step 1600 learning rate 0.3000 step-time 6.21 perplexity 4.01
  eval: bucket 0 perplexity 2.49
  eval: bucket 1 perplexity 4.28
  eval: bucket 2 perplexity 7.46
  eval: bucket 3 perplexity 18.77
max_preplexity_count: 0 
 prexs: [2.489708961213821, 4.278402380095374, 7.458116142707525, 18.774568663767635]
global step 2000 learning rate 0.3000 step-time 6.43 perplexity 3.21
  eval: bucket 0 perplexity 2.98
  eval: bucket 1 perplexity 4.02
  eval: bucket 2 perplexity 4.68
  eval: bucket 3 perplexity 34.32
max_preplexity_count: 0 
 prexs: [2.9773611635105324, 4.015599564737443, 4.679015482775286, 34.31750241461236]
global step 2400 learning rate 0.3000 step-time 6.52 perplexity 2.96
  eval: bucket 0 perplexity 2.29
  eval: bucket 1 perplexity 3.80
  eval: bucket 2 perplexity 6.02
  eval: bucket 3 perplexity 27.87
max_preplexity_count: 0 
 prexs: [2.292797006269657, 3.7983932807808363, 6.02019854249518, 27.87291183984407]
global step 2800 learning rate 0.3000 step-time 6.48 perplexity 2.53
  eval: bucket 0 perplexity 1.43
  eval: bucket 1 perplexity 3.54
  eval: bucket 2 perplexity 4.98
  eval: bucket 3 perplexity 19.04
max_preplexity_count: 0 
 prexs: [1.4304824745673652, 3.5375095619959174, 4.980616243293388, 19.035592575200813]
global step 3200 learning rate 0.3000 step-time 6.30 perplexity 2.12
  eval: bucket 0 perplexity 4.34
  eval: bucket 1 perplexity 2.47
  eval: bucket 2 perplexity 4.20
  eval: bucket 3 perplexity 33.89
max_preplexity_count: 0 
 prexs: [4.3394637810907835, 2.470602331618613, 4.198687660250878, 33.88573202064156]
global step 3600 learning rate 0.3000 step-time 6.46 perplexity 2.01
  eval: bucket 0 perplexity 1.60
  eval: bucket 1 perplexity 2.27
  eval: bucket 2 perplexity 6.61
  eval: bucket 3 perplexity 27.70
max_preplexity_count: 0 
 prexs: [1.6012587337909885, 2.270746004155892, 6.605633747519744, 27.698036930530293]
global step 4000 learning rate 0.3000 step-time 6.34 perplexity 1.99
  eval: bucket 0 perplexity 1.78
  eval: bucket 1 perplexity 2.42
  eval: bucket 2 perplexity 2.51
  eval: bucket 3 perplexity 24.99
max_preplexity_count: 0 
 prexs: [1.7796986101157823, 2.42387850006293, 2.51157069385073, 24.988810256205117]
global step 4400 learning rate 0.3000 step-time 6.32 perplexity 1.69
  eval: bucket 0 perplexity 1.42
  eval: bucket 1 perplexity 2.03
  eval: bucket 2 perplexity 3.23
  eval: bucket 3 perplexity 24.35
max_preplexity_count: 0 
 prexs: [1.4225767807827807, 2.025727794964597, 3.231349469059865, 24.35051607539681]
global step 4800 learning rate 0.3000 step-time 6.56 perplexity 1.63
  eval: bucket 0 perplexity 1.23
  eval: bucket 1 perplexity 2.10
  eval: bucket 2 perplexity 2.24
  eval: bucket 3 perplexity 19.72
max_preplexity_count: 0 
 prexs: [1.2330244253103255, 2.1043234153316788, 2.2438631219188134, 19.71765552432555]
global step 5200 learning rate 0.3000 step-time 6.48 perplexity 1.60
  eval: bucket 0 perplexity 1.22
  eval: bucket 1 perplexity 1.87
  eval: bucket 2 perplexity 2.51
  eval: bucket 3 perplexity 25.40
max_preplexity_count: 0 
 prexs: [1.2189742354700936, 1.866775633105441, 2.510714548489428, 25.404945676141146]
global step 5600 learning rate 0.3000 step-time 6.26 perplexity 1.48
  eval: bucket 0 perplexity 1.67
  eval: bucket 1 perplexity 1.85
  eval: bucket 2 perplexity 1.44
  eval: bucket 3 perplexity 32.24
max_preplexity_count: 0 
 prexs: [1.6709173364703733, 1.8496985119602158, 1.4360749760977392, 32.23843303104762]
global step 6000 learning rate 0.3000 step-time 6.03 perplexity 1.46
  eval: bucket 0 perplexity 1.58
  eval: bucket 1 perplexity 1.64
  eval: bucket 2 perplexity 1.51
  eval: bucket 3 perplexity 31.96
max_preplexity_count: 0 
 prexs: [1.5819616683142572, 1.639585272425461, 1.5093290301612508, 31.963323950819102]
global step 6400 learning rate 0.3000 step-time 6.29 perplexity 1.42
  eval: bucket 0 perplexity 1.40
  eval: bucket 1 perplexity 1.60
  eval: bucket 2 perplexity 1.49
  eval: bucket 3 perplexity 31.98
max_preplexity_count: 0 
 prexs: [1.3983256972917617, 1.603384358463247, 1.488740474411764, 31.976479899921625]
global step 6800 learning rate 0.3000 step-time 6.00 perplexity 1.35
  eval: bucket 0 perplexity 1.16
  eval: bucket 1 perplexity 1.31
  eval: bucket 2 perplexity 1.36
  eval: bucket 3 perplexity 25.73
max_preplexity_count: 0 
 prexs: [1.155763098654922, 1.3055864357054923, 1.3614236132421853, 25.73029830703974]
global step 7200 learning rate 0.3000 step-time 6.31 perplexity 1.34
  eval: bucket 0 perplexity 1.06
  eval: bucket 1 perplexity 1.64
  eval: bucket 2 perplexity 1.28
  eval: bucket 3 perplexity 38.68
max_preplexity_count: 0 
 prexs: [1.0628425830795452, 1.6402763960774398, 1.283006237048694, 38.679469750468414]
global step 7600 learning rate 0.3000 step-time 6.50 perplexity 1.31
  eval: bucket 0 perplexity 1.72
  eval: bucket 1 perplexity 1.34
  eval: bucket 2 perplexity 1.24
  eval: bucket 3 perplexity 24.80
max_preplexity_count: 0 
 prexs: [1.7247197413816664, 1.335414938603433, 1.2409340107203797, 24.804474144316796]
global step 8000 learning rate 0.3000 step-time 6.68 perplexity 1.28
  eval: bucket 0 perplexity 1.18
  eval: bucket 1 perplexity 1.67
  eval: bucket 2 perplexity 1.19
  eval: bucket 3 perplexity 34.89
max_preplexity_count: 0 
 prexs: [1.1838731024137639, 1.6710519935711632, 1.1936117606504297, 34.887872717856744]
global step 8400 learning rate 0.3000 step-time 6.53 perplexity 1.28
  eval: bucket 0 perplexity 1.04
  eval: bucket 1 perplexity 1.39
  eval: bucket 2 perplexity 1.20
  eval: bucket 3 perplexity 10.43
max_preplexity_count: 0 
 prexs: [1.0396806747524494, 1.3913618088992608, 1.1965535536269305, 10.43443972227012]
global step 8800 learning rate 0.3000 step-time 6.41 perplexity 1.26
  eval: bucket 0 perplexity 1.95
  eval: bucket 1 perplexity 1.76
  eval: bucket 2 perplexity 1.29
  eval: bucket 3 perplexity 13.19
max_preplexity_count: 0 
 prexs: [1.9450154321167974, 1.7568755954597528, 1.2874764814593225, 13.187097528458768]
global step 9200 learning rate 0.3000 step-time 6.25 perplexity 1.22
  eval: bucket 0 perplexity 1.19
  eval: bucket 1 perplexity 1.47
  eval: bucket 2 perplexity 1.27
  eval: bucket 3 perplexity 14.94
max_preplexity_count: 0 
 prexs: [1.1880844631207783, 1.4678629970024202, 1.2653988136341243, 14.940830210451457]
global step 9600 learning rate 0.3000 step-time 6.42 perplexity 1.25
  eval: bucket 0 perplexity 1.29
  eval: bucket 1 perplexity 1.12
  eval: bucket 2 perplexity 1.06
  eval: bucket 3 perplexity 11.48
max_preplexity_count: 0 
 prexs: [1.2928823610812388, 1.1196610026729181, 1.0639565964909998, 11.4817131137831]
global step 10000 learning rate 0.3000 step-time 6.12 perplexity 1.23
  eval: bucket 0 perplexity 1.30
  eval: bucket 1 perplexity 1.15
  eval: bucket 2 perplexity 1.06
  eval: bucket 3 perplexity 11.63
max_preplexity_count: 0 
 prexs: [1.2975129543306043, 1.145856374019046, 1.0560446316506171, 11.633661175346093]
global step 10400 learning rate 0.3000 step-time 6.30 perplexity 1.20
  eval: bucket 0 perplexity 1.01
  eval: bucket 1 perplexity 1.44
  eval: bucket 2 perplexity 1.12
  eval: bucket 3 perplexity 11.87
max_preplexity_count: 0 
 prexs: [1.0128893244916128, 1.4397558084020485, 1.1227819285778455, 11.868939660353513]
